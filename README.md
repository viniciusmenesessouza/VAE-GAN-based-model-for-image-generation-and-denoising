# VAE-GAN-based-model-for-image-generation-and-denoising
This practical project aims to develop a deep learning model capable of generating synthetic images and performing image denoising. This practical project aims to develop a deep learning model capable of generating synthetic images and performing image denoising. To achieve this, we propose using a known architecture, which combines the representational strength of a Variational Autoencoder (VAE) with the image generation quality of a Generative Adversarial Network (GAN), called VAE-GAN [1]. The model comprises three key components (see Fig. 1a): (i) an encoder, (ii) a decoder/generator, and (iii) a discriminator. The encoder processes noisy input images and maps them to a meaningful latent representation. The generator then reconstructs images from this latent space. Depending on the input, the latent vector can either be sampled randomly—for synthetic image generation (see Fig. 1b)—or obtained from the encoder to perform image denoising (see Fig. 1c). The discriminator is used during training to distinguish between real and generated images, encouraging the generator to produce realistic outputs

![training_page-0001](https://github.com/user-attachments/assets/634038b2-a30a-4156-8125-25d9efe656e7)

## Methodology

To carry out this project, the VAE-GAN model will be implemented using PyTorch [1]. Each component consists of a Convolutional Neural Network, with its configuration adapted to a specific task (upsampling for the decoder/generator, downsampling for the encoder and discriminator). The GAN loss, <img src="https://latex.codecogs.com/png.latex?\fg{BBBBBB}\mathcal{L}_{\text{GAN}}" />, uses a binary cross-entropy loss with respect to Discriminator/Generator output, while the VAE loss, <img src="https://latex.codecogs.com/gif.latex?\mathcal{L}_{\text{VAE}}" />, uses log-likelihood (expressed in GAN discriminator) and a regularization term based on Kullback-Leibler divergence (<img src="https://latex.codecogs.com/gif.latex?\text{D}_{\text{KL}}" />). For the data sample x, with latent representation z, the mentioned losses are given by:

<p align="center">
  <img src="https://latex.codecogs.com/png.latex?\dpi{200}\bg{white}\begin{matrix*}\color{white}{000000000000000000000000000000000000000000000}\\\mathcal{L}=\mathcal{L}_{\text{Dis}_l}+\mathcal{L}_{\text{prior}}+\mathcal{L}_{\text{GAN}}\\\mathcal{L}_{\text{GAN}}=\log(\text{Dis}(x))+\log(1-\text{Dis}(\text{Gen}(z)))\\\mathcal{L}_{\text{Dis}_l}=-\mathbb{E}_{q(z|x)}\left[\log{p(\text{Dis}_l(x)|z)}\right]\\\mathcal{L}_{\text{prior}}=\text{D}_{\text{KL}}(q(z|x)\|p(z))\\\color{white}{000000000000000000000000000000000000000000000}\end{matrix*}"/>
</p>

Where <img src="https://latex.codecogs.com/gif.latex?\text{Dis}_l(x)" /> is the representation at the l-th hidden layer of the discriminator. In the original proposal, Larsen et al. [2] present an algorithm for training these models: first, the input sample/batch is encoded and decoded by the first two components, and the loss between them is computed; samples from a prior <img src="https://latex.codecogs.com/gif.latex?p(z)\sim\mathcal{N}(0,\mathbf{I})" /> are then passed through the decoder to generate an artificial input; finally, the original input, encoded-decoded input, and artificial input are passed to the discriminator to calculate the GAN loss, and parameters for the three components are updated according to the gradients. Initially, a well-established dataset will be used to assess the validity of the implementation and the impact of several hyperparameters on overall performance, namely convolutional layer size across the different components. After this, a VAE-GAN model will be trained on datasets relevant to other applications.
